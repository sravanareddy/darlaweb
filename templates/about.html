{% extends "layout.html" %}
{% block title %}About{% endblock %}
{% block content %}
<div class="container">

  <h2>FAQs</h2>


  <ol class="list-group">
    <li class="list-group-item">
    <a href="#" data-toggle="collapse" data-target="#whatbody"><h3
    id="what">
    What is DARLA?</h3></a>

    <div id="whatbody" class="collapse">
      <p>DARLA is a web application providing two main functionalities for vowel
    extraction from speech: completely
    automated and semi-automated.</p>

<p>The completely automated system transcribes the input
    speech data using automatic speech recognition (ASR), and then
    runs it through forced alignment and formant extraction.</p>

    <p>The semi-automated system is a <a
    href="http://fave.ling.upenn.edu/">FAVE-based</a> approach that
    aligns and extracts vowel formants from speech with manual
      transcriptions.</p></div>
      </li>

      <li class="list-group-item">

      <a href="#" data-toggle="collapse" data-target="#outputbody"><h3
      id="output">What output does the system return?</h3></a>

       <div id="outputbody" class="collapse">
<p>We e-mail a set of files to you: a vowel plot showing the mean of all vowels
  in your data (including both stressed and unstressed vowels)
and a spreadsheet with both unnormalized and Lobanov-normalized
formant measurements,
the same spreadsheet formatted for convenient uploading to <a
href="http://lvc.uoregon.edu/norm/about_norm1.php">NORM</a>, the
alignments, and the transcriptions.
You can select whether you want to filter out stop words
<a href="/static/content/stopwords.txt">(from this list)</a>
and high-bandwidth vowels.
</p>

<p>We recommend remove the unstressed vowels from your spreadsheet
  output before plotting it in NORM.
</p>

<p>If you are using the completely automated feature, you also have the option of
editing the transcriptions online, and seamlessly re-running your task
	 through the system.</p>
       </div>

    </li>

    <li class="list-group-item">
    <a href="#" data-toggle="collapse" data-target="#asrbody">
    <h3 id="asr">What ASR is used for the completely automated feature?</h3></a>

     <div id="asrbody" class="collapse">
    <p>We use the <a href="https://cloud.google.com/speech/">Google Speech API</a>,
    one of the current state-of-art technologies in speech recognition.</p>
     </div></li>

       <li class="list-group-item">
  <a href="#asr" data-toggle="collapse" data-target="#evalbody">
      <h3 id="eval">How can I evaluate the accuracy of the ASR transcriptions?</h3></a>

  <div id="evalbody" class="collapse">
    <p>
    <a href="uploadeval">Our online transcription evaluation tool</a>
    uses the weighted Levenshtein distance algorithm to compute
   transcription error rates for words, phonemes, and stressed vowels. Simply
    upload the ASR transcription and the manual transcription in
    plaintext format.</p>

</div></li>



  <li class="list-group-item">
  <a href="#eval" data-toggle="collapse" data-target="#techbody">
      <h3 id="tech">What do you use for alignment and extraction?</h3></a>

  <div id="techbody" class="collapse">
<p>We use the same methods as FAVE. Our forced alignment is done with the
  <a href="http://prosodylab.org/tools/aligner/">ProsodyLab
  Aligner</a>, and formant measurement with <a
  href="https://github.com/JoFrhwld/FAVE/tree/master/FAVE-extract">FAVE-extract</a>.
  We also use the <a href="http://cran.r-project.org/web/packages/vowels/index.html">R vowels package</a> for plotting.</p>
</div></li>

    <li class="list-group-item">
    <a href="#tech" data-toggle="collapse" data-target="#whybody">
    <h3 id="why">Why completely automate vowel extraction?</h3>
    </a>

     <div class="collapse" id="whybody">
<p>In the last few years, sociolinguists have begun using semi-automated speech processing methods such as Penn's <a href="http://fave.ling.upenn.edu/index.html">FAVE</a> program to extract vowel formants. These systems have accelerated the pace of linguistic research, but require significant human effort to manually create sentence-level transcriptions. </p>

<p>We believe that sociolinguistics is on the brink of an even more
  transformative technology: large-scale, completely automated vowel
  extraction without any need for human transcription. This technology
  would make it possible to quickly extract pronunciation features
  from hours of recordings, including YouTube and vast audio
    archives. DARLA takes a step in this direction. </p>
  </div>

      </li>

      <li class="list-group-item">
<a href="#why" data-toggle="collapse" data-target="#errorsbody">
      <h3 id="errors">How do we deal with ASR errors?</h3></a>

      <div class="collapse" id="errorsbody">
<p>While ASR is far from perfect, we believe sociophoneticians do not need to wait for years to take advantage of speech recognition. Unlike applications like dictation software where accurate word recognition is the primary goal, sociophonetics typically focuses on a much narrower objective: extracting a representative vowel space for speakers, based on stressed vowel tokens.
For example, it would usually not be crucial to know that the stressed vowel in the word "turning" was extracted from "turn it" rather than "turning", or that "tack" was wrongly transcribed as "sack".  Such differences will have little effect on the speaker's vowel space for many sociophonetic questions. </p>

<p>It turns out that most ASR errors affect the identity of the <i>words</i> but not the identity of the <i>vowels</i> (especially stressed vowels), making it an ideal technology for automated vowel analysis.
Of course, there will be instances of vowel error, but the effect of these errors is reduced by the large amount of data with hundreds or thousands of vowel tokens.</p>

<p>Important contrasts like "cot" versus "caught" tend to be handled by ASR's modeling of grammatical plausibility (using a <a href="http://en.wikipedia.org/wiki/Language_model">language model</a>). The system would be unlikely to transcribe "I caught the ball" as "I cot the ball" since the latter would be improbable under an English language model.</p>

<p>Finally, since DARLA shows probabilities for the phonetic
	environment around each vowel (e.g., obstruent+vowel+nasal
	consonant), researchers can examine contrasts like pin/pen
	versus pit/pet.</p>
      </div>
      </li>

      <li class="list-group-item">

      <a href="#errors" data-toggle="collapse" data-target="#limitsbody">
      <h3 id="limits">Sounds great! What's the catch with the completely automated system?</h3></a>

       <div class="collapse" id="limitsbody">
<p> DARLA's completely automated system cannot provide perfect transcriptions. The automatic
transcriptions typically contain a very large number of errors,
especially in free speech data, even using Google's ASR.
Current technology cannot match the accuracy of human manual transcriptions.</p>

<p>DARLA's completely automated approach to sociophonetics may help open the way
toward large-scale audio analysis, but there is a tradeoff. As with
many other sciences, automated processing necessitates error-reporting
in the measurements, not just in the statistical modeling. We find
that the system can be useful for extracting a representative vowel
	 space for sociophonetic purposes, as long as error levels are
	 considered and reported. In other words, fast large-scale
	 data analysis requires a higher tolerance of noise in the
	 data. In you need greater accuracy, please use our semi-automated system instead.</p>
       </div>

       </li>


       <li class="list-group-item">
       <a href="#limits" data-toggle="collapse" data-target="#semibody">
       <h3 id="semi">What is the semi-automated functionality? How
       does it differ from FAVE and other such tools?</h3></a>

       <div class="collapse" id="semibody">
<p>This is designed for research that requires accurate human
transcription.
Our semi-automated system relies heavily on code from ProsodyLab
	 Aligner and FAVE, but
	 wraps it in a different interface and provides more output features.</p>

<p>DARLA allows you to upload your transcriptions in various
formats: as a plaintext file, or as a TextGrid with a pair of
boundaries around each transcribed sentence (the "Boundaries" option).
You can also upload manually aligned/corrected TextGrids for formant
	 extraction only. Another option is to use the completely
	 automated system to generate ASR transcriptions, and then
	 correct them using our online tool.</p>

	 <p>Some steps that require manual intervention
	 in FAVE, like creating pronunciations of words that are not
	 in the dictionary, are automated.</p>

<p> For research requiring perfect transcriptions:
<ol>
<li>
Use the <a href="googlespeech">completely automated ASR</a> option
as a first pass and then correct the transcriptions
online with our playback tool, OR

<li>If you can spend the time to produce manual transcriptions,
using the semi-automated <a href="uploadboundtrans">Boundaries</a> option.
The semi-automated <a href="uploadtxttrans">plaintext</a>
method works just as well, but you will need to delete noises,
laughter, interviewer's voice, etc. With the Boundaries method, such
deletions aren't necessary since you are simply putting boundaries
around the parts of the recording that you want.</p>
</ol>
</p>
       </div>

            <li class="list-group-item">

      <a href="#semi" data-toggle="collapse" data-target="#noisebody">
      <h3 id="noise">What about noise or multiple voices in the recording?</h3></a>

       <div class="collapse" id="noisebody">

	 <p>The system cannot currently handle recordings with
	 multiple speakers in an automated way (though this is an idea
	 for future work). </p>

	 	<p>When DARLA processes a recording with noise, laughter, loud breaths, background
	 voices, music, etc., the ASR transcriptions or
      alignments are likely to be incorrect.
	 If your recording would require a great deal of pre-cleaning,
      you might want to consider manually transcribing with the
      semi-automated method rather than the completely automated
	 one.</p>

	<p>However, it is easy to manually delete extraneous sounds and
	voices in Praat (select the noise and
	click Cmd+X or Ctrl+X). </p>

	<p> If your recording includes an interviewer who doesn't have
	 a microphone, this quiet background voice can cause confusion
	 for the ASR and aligner.
	 The best solution is to delete the interviewer voice (see
	 above), but here are some other options:</p>

	 <ol>
	   <li> Try "smoothing out" the amplitude of all voices on the
	 recording: Load your file in Audacity, then click Effects >
	   Compressor.
	   That feature is a dynamic range adjuster which tries to
	   make all voices approximately the same
	   (pull the slider all the way to the left for the strongest effect). </li>
<li> Try reducing the amplitude of the whole recording (in Audacity,
	 click Effects > Amplify) so that the quieter voice is
	   non-existent.</li>
	   This may help remove quiet background voices that would be a problem for the aligner.
<li> You can also try increasing the amplitude of all voices so that
	 the ASR transcription can "hear" all of them clearly: In
	   Audacity, click Effects > Amplify.</li>
	  </ol>
	</div>
	</li>

	  <li class="list-group-item">
    <a href="#noise" data-toggle="collapse" data-target="#codebody"><h3
    id="code">
    Is the code for DARLA public? Can I contribute?</h3></a>

    <div id="codebody" class="collapse">
      <p><a href="#eval" data-toggle="collapse"
      data-target="#techbody">See this question</a> for links to the alignment,
      extraction, and plotting code written by other researchers that we are
      building upon.

<p>Our code for the rest of the system -- the web interface, the ASR,
      the YouTube features, online correction and evaluation of transcripts,
      handling different file formats,
      etc. -- is currently not public because
      it is under active development. We also think that most users
      prefer the convenience of the web interface rather than
      installing and wrangling with several programs on their computers.</p>

      <p>If you are interested in contributing a new feature or
      modifying an existing functionality, please e-mail us! We are
      excited to collaborate on related linguistics and computer science
      research, as well as the software
      development front.
    </div>
      </li>


    </ul>   <!-- ends list-group ul -->



</div>   <!-- ends container -->
{% endblock %}
